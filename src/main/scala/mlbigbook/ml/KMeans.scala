package mlbigbook.ml

import java.util.Random

import mlbigbook.data._

import scala.annotation.tailrec

/** Input for the KMeans clustering algorithm.
  *
  * The Distance function (d) controls the k-means algorithm's notion of closeness
  * and farness. The nClusters value is the k in k-means; it's the number of clusters.
  *
  * The tolerance and maxIterations values describe the stopping conditions of the
  * k-means algorithm. If the absolute value of the sum of the differences in
  * cluster centers between two iterations is less than tolerance, then the algorithm
  * halts. Or, if the number of iterations exceeds maxIterations, then the algorithm
  * halts.
  */
case class KMeansIn(d: Distance, nClusters: Int, tolerance: Double, maxIterations: Int)

/** Evaluated result from clustering. Describes indexed clusters.
  *
  * The Vectorizer[T] type (v) allows one to make vectors that will be in the
  * same space as these cluster centers. The cardinality is the size of the
  * vectorspace for all of the clusters. The centers value is this indexed
  * sequence of cluster centers.
  */
case class VectorizedCenters[T]
(cardinality: Int, v: Vectorizer[T], centers: IndexedSeq[Center])

/** Represents a cluster center.
  *
  * The id value is helpful for naming clusters. The mean value is the vector
  * representation of the cluster's center.
  */
case class Center(id: String, mean: Vector)

object KMeans {

  /** Creates cluster centers using the K-Means algorithm. */
  def apply[T](k: KMeansIn)(vdata: VectorDataIn[T])(implicit rand: Random): VectorizedCenters[T] = {

    val (vectorizer, vectorized) = vdata()

    val cardinality = vectorized.take(1).toSeq.headOption.map(_._2.cardinality).getOrElse(0)

    val initialCenters = VectorizedCenters[T](
      cardinality,
      vectorizer,
      (0 until k.nClusters).map(id =>
        Center(id.toString, DenseVector.mkRandom(cardinality))
      )
    )

    if (cardinality != 0)
      apply_h(k, initialCenters, 0.0, 0, vectorized)
    else
      initialCenters
  }

  /*
    :: from Wikipedia page on k-means ::

    ===
    Assignment step:
    ===
    Assign each observation to the cluster whose mean yields the least within-cluster
    sum of squares (WCSS). Since the sum of squares is the squared Euclidean distance,
    this is intuitively the "nearest" mean.[8] (Mathematically, this means partitioning
    the observations according to the Voronoi diagram generated by the means).

    val initialTol = calculateTolerance()
      S_i^{(t)} = \big \{ x_p : \big \| x_p - m^{(t)}_i \big \|^2 \le \big \| x_p - m^{(t)}_j \big \|^2 \ \forall j, 1 \le j \le k \big\},

    ===
    Update step:
    ===
    Calculate the new means to be the centroids of the observations in the new clusters.

        m^{(t+1)}_i = \frac{1}{|S^{(t)}_i|} \sum_{x_j \in S^{(t)}_i} x_j
    */
  @tailrec @inline private def apply_h[T](
    k: KMeansIn,
    current: VectorizedCenters[T],
    currTol: Double,
    currIter: Int,
    data: DistData[(T, Vector)]): VectorizedCenters[T] =

    if (currIter >= k.maxIterations) {
      current

    } else {

      val updated = updateCenters(k, data, current)

      val updatedTol = calculateTolerance(current.centers, updated.centers)

      if (currIter > 0 && Math.abs(currTol - updatedTol) >= k.tolerance)
        updated
      else
        apply_h(k, updated, updatedTol, currIter + 1, data)
    }

  def updateCenters[T](
    k: KMeansIn,
    data: DistData[(T, Vector)],
    current: VectorizedCenters[T]): VectorizedCenters[T] = {

    val (newCenterBuilders, nVecs) =
      // assign each vector in the data to a cluster center using the current centers
      assignment(HardCluster(k.d)(current))(data)
        // create empty dense vector builders
        // and
        // aggregate each new center by summing all vectors that are assigned to a center
        // also keep track of the number of vectors we see
        .aggregate((mkCenterBuilders(current), 0.0))(
          {
            case ((cbs, n), (assignedCenter, vector)) =>
              // mutates the DenseVectorBuilder that's in the mapping
              cbs(assignedCenter.label)._1.add(vector)
              // keep the same mapping, increase the # of observed instances
              (cbs, n + 1)
          },
          {
            case ((cbs1, n1), (cbs2, n2)) =>

              val updatedCB =
                cbs1.keys.foldLeft(cbs2)({
                  case (updatingCbs2, id) =>
                    // mutates dense vector builder in mapping
                    updatingCbs2(id)._1.add(cbs1(id)._1.create)
                    // keep mapping the same
                    updatingCbs2
                })

              (updatedCB, n1 + n2)
          })

    val newCenters =
      newCenterBuilders.toIndexedSeq
        // get the centers back in their original order
        .sortBy(_._2._2)
        .map({
          case (id, centerBuilder) =>
            // mutates the dense vector builder
            // divide the summed vector by the # of observed vectors to obtain
            // the mean: the new, updated, center
            centerBuilder._1.normalize(nVecs)
            // construct a side-effect free vector from this builder
            Center(id, centerBuilder._1.create)
        })

    current.copy[T](centers = newCenters)
  }

  def assignment[T](c: Classifier[T])(data: DistData[(T, Vector)]): DistData[(Labeled, Vector)] =
    data.map({ case (item, vector) => (c(item), vector) })

  def mkCenterBuilders(prevCenters: VectorizedCenters[_]): Map[String, (DenseVectorBuilder, Int)] =
    prevCenters.centers.zipWithIndex
      .map({
        case (c, index) =>
          (c.id, (DenseVectorBuilder(prevCenters.cardinality), index))
      }).toMap

  def calculateTolerance(prev: IndexedSeq[Center], curr: IndexedSeq[Center]): Double =
    prev.zip(curr)
      .foldLeft(0.0)({
        case (tol, (oldCenter, newCenter)) =>
          tol + Vector.absElemDiff(oldCenter.mean, newCenter.mean)
      })

}